<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Self-Evolving Agents - Evolution Methods</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            min-height: 100vh;
            color: #fff;
        }
        .header {
            padding: 16px 24px;
            background: rgba(255,255,255,0.03);
            border-bottom: 1px solid rgba(255,255,255,0.08);
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .header h1 {
            font-size: 1.3rem;
            font-weight: 600;
            background: linear-gradient(135deg, #3498DB, #2ECC71);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        .header-controls {
            display: flex;
            align-items: center;
            gap: 20px;
        }
        .year-display {
            font-size: 1.8rem;
            font-weight: 700;
            color: #3498DB;
            min-width: 70px;
            text-align: center;
            font-family: 'SF Mono', Monaco, monospace;
        }
        .play-btn {
            width: 40px;
            height: 40px;
            border-radius: 50%;
            background: rgba(52,152,219,0.15);
            border: 2px solid rgba(52,152,219,0.4);
            color: #3498DB;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.2s;
        }
        .play-btn:hover {
            background: rgba(52,152,219,0.25);
            border-color: #3498DB;
        }
        .play-btn svg { width: 18px; height: 18px; }
        .stats {
            display: flex;
            gap: 16px;
            font-size: 0.8rem;
            color: rgba(255,255,255,0.6);
        }
        .stat-value { color: #3498DB; font-weight: 600; }
        .container {
            display: flex;
            height: calc(100vh - 65px);
        }
        .graph-container {
            flex: 1;
            position: relative;
        }
        #graph { width: 100%; height: 100%; }
        .sidebar {
            width: 340px;
            background: rgba(0,0,0,0.3);
            border-left: 1px solid rgba(255,255,255,0.08);
            display: flex;
            flex-direction: column;
        }
        .legend-section {
            padding: 14px;
            border-bottom: 1px solid rgba(255,255,255,0.08);
        }
        .legend-title {
            font-size: 0.65rem;
            text-transform: uppercase;
            letter-spacing: 1.5px;
            color: rgba(255,255,255,0.4);
            margin-bottom: 10px;
        }
        .legend {
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
        }
        .legend-item {
            display: flex;
            align-items: center;
            font-size: 0.7rem;
            background: rgba(255,255,255,0.05);
            padding: 5px 10px;
            border-radius: 14px;
            cursor: pointer;
            transition: all 0.2s;
        }
        .legend-item:hover {
            background: rgba(255,255,255,0.1);
        }
        .legend-item.active {
            background: rgba(255,255,255,0.15);
        }
        .legend-color {
            width: 10px;
            height: 10px;
            border-radius: 50%;
            margin-right: 6px;
        }
        .detail-panel {
            flex: 1;
            overflow-y: auto;
            padding: 16px;
        }
        .detail-empty {
            height: 100%;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            color: rgba(255,255,255,0.3);
            text-align: center;
            padding: 30px;
        }
        .detail-empty p { font-size: 0.85rem; }
        .method-card { animation: fadeIn 0.2s ease; }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(6px); }
            to { opacity: 1; transform: translateY(0); }
        }
        .method-name {
            font-size: 1.15rem;
            font-weight: 600;
            color: #3498DB;
            margin-bottom: 4px;
        }
        .method-category {
            display: inline-block;
            font-size: 0.65rem;
            color: rgba(255,255,255,0.9);
            text-transform: uppercase;
            letter-spacing: 1px;
            padding: 3px 8px;
            border-radius: 10px;
            margin-bottom: 14px;
        }
        .info-box {
            background: rgba(255,255,255,0.04);
            border-radius: 8px;
            padding: 12px;
            margin-bottom: 10px;
        }
        .info-box h4 {
            font-size: 0.6rem;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: rgba(255,255,255,0.4);
            margin-bottom: 5px;
        }
        .info-box p {
            font-size: 0.85rem;
            line-height: 1.45;
            color: rgba(255,255,255,0.85);
        }
        .info-box.how { border-left: 3px solid #2ECC71; }
        .info-box.what { border-left: 3px solid #3498DB; }
        .stats-row {
            display: flex;
            gap: 12px;
            margin-bottom: 14px;
        }
        .stat-box {
            flex: 1;
            background: rgba(255,255,255,0.04);
            border-radius: 8px;
            padding: 10px;
            text-align: center;
        }
        .stat-box .num {
            font-size: 1.3rem;
            font-weight: 700;
            color: #3498DB;
        }
        .stat-box .lbl {
            font-size: 0.55rem;
            color: rgba(255,255,255,0.4);
            text-transform: uppercase;
        }
        .papers-title {
            font-size: 0.6rem;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: rgba(255,255,255,0.4);
            margin: 14px 0 8px;
        }
        .paper-item {
            background: rgba(255,255,255,0.03);
            border-left: 2px solid #3498DB;
            padding: 8px 10px;
            margin-bottom: 6px;
            border-radius: 0 6px 6px 0;
        }
        .paper-title {
            font-size: 0.72rem;
            color: rgba(255,255,255,0.85);
            line-height: 1.35;
        }
        .paper-year {
            font-size: 0.62rem;
            color: #3498DB;
            margin-top: 3px;
        }
        .tooltip {
            position: absolute;
            background: rgba(10,10,20,0.95);
            border: 1px solid rgba(52,152,219,0.4);
            border-radius: 8px;
            padding: 12px;
            font-size: 0.78rem;
            pointer-events: none;
            opacity: 0;
            max-width: 280px;
            z-index: 1000;
            box-shadow: 0 6px 24px rgba(0,0,0,0.5);
        }
        .tooltip.visible { opacity: 1; }
        .tooltip h5 { color: #3498DB; margin-bottom: 4px; font-size: 0.88rem; }
        .tooltip .cat { color: rgba(255,255,255,0.5); font-size: 0.65rem; margin-bottom: 5px; }
        .tooltip .how { color: #2ECC71; font-style: italic; font-size: 0.72rem; }
        .tooltip .freq { color: rgba(255,255,255,0.6); font-size: 0.68rem; margin-top: 4px; }
        .controls {
            position: absolute;
            bottom: 14px;
            left: 14px;
        }
        .controls button {
            padding: 7px 12px;
            background: rgba(52,152,219,0.12);
            border: 1px solid rgba(52,152,219,0.3);
            border-radius: 5px;
            color: #3498DB;
            cursor: pointer;
            font-size: 0.72rem;
        }
        .node { cursor: pointer; }
        .node circle {
            stroke-width: 2px;
            transition: all 0.25s;
        }
        .node.dimmed circle { opacity: 0.12; }
        .node.dimmed text { opacity: 0.12; }
        .node:hover circle, .node.selected circle {
            stroke: #fff;
            stroke-width: 3px;
            filter: drop-shadow(0 0 12px rgba(52,152,219,0.6));
        }
        .node.highlighted circle {
            stroke: #fff;
            stroke-width: 3px;
            filter: drop-shadow(0 0 16px rgba(52,152,219,0.8));
        }
        .node text {
            font-size: 9px;
            fill: rgba(255,255,255,0.8);
            pointer-events: none;
            font-weight: 500;
        }
        .category-node circle {
            fill-opacity: 0.18;
            stroke-width: 2.5px;
        }
        .category-node text {
            font-size: 11px;
            font-weight: 700;
            letter-spacing: 0.5px;
            text-transform: uppercase;
            fill: rgba(255,255,255,0.7);
        }
        .link {
            stroke: rgba(255,255,255,0.08);
            stroke-width: 1px;
        }
        .link.category-link {
            stroke: rgba(255,255,255,0.18);
            stroke-dasharray: 2 3;
        }
        .link.dimmed { opacity: 0.03; }
        .link.highlighted {
            stroke: rgba(52,152,219,0.4);
            stroke-width: 2px;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Self-Evolving Agent Methods</h1>
        <div class="header-controls">
            <div class="stats">
                <span><span class="stat-value">37</span> methods</span>
                <span><span class="stat-value">7</span> connections</span>
            </div>
            <button class="play-btn" id="playBtn" title="Animate by year">
                <svg viewBox="0 0 24 24" fill="currentColor"><path d="M8 5v14l11-7z"/></svg>
            </button>
            <div class="year-display" id="yearDisplay">ALL</div>
        </div>
    </div>
    <div class="container">
        <div class="graph-container">
            <svg id="graph"></svg>
            <div class="tooltip" id="tooltip"></div>
            <div class="controls">
                <button onclick="resetZoom()">Reset View</button>
            </div>
        </div>
        <div class="sidebar">
            <div class="legend-section">
                <div class="legend-title">Evolution Method Types</div>
                <div class="legend" id="legend"></div>
            </div>
            <div class="detail-panel" id="detailPanel">
                <div class="detail-empty">
                    <p>Click a method node to see<br>how it enables agent evolution</p>
                </div>
            </div>
        </div>
    </div>
    <script>
        const data = {
            nodes: [{"id": "Workflow Optimization", "category": "Multi-Agent", "how": "Evolve agent communication patterns and routing", "what": "Optimizes how agents coordinate", "frequency": 10, "years": [2024, 2025], "yearRange": "2024-2025", "papers": [{"arxiv_id": "2409.07429", "title": "Agent Workflow Memory", "year": 2024, "context": ""}, {"arxiv_id": "2410.10762", "title": "AFlow: Automating Agentic Workflow Generation", "year": 2024, "context": "aflow: automating agentic workflow generation"}, {"arxiv_id": null, "title": "WorkflowLLM: Enhancing Workflow Orchestration Capability of Large Language Models", "year": 0, "context": "workflowllm: enhancing workflow orchestration capability of large language models"}, {"arxiv_id": null, "title": "Flow: Modularized Agentic Workflow Automation", "year": 0, "context": "flow: modularized agentic workflow automation"}, {"arxiv_id": null, "title": "FlowAgent: Achieving Compliance and Flexibility for Workflow Agents", "year": 0, "context": "flowagent: achieving compliance and flexibility for workflow agents"}, {"arxiv_id": "2502.04306", "title": "ScoreFlow: Mastering LLM Agent Workflows via Score-Based Preference Optimization", "year": 2025, "context": "scoreflow: mastering llm agent workflows via score-based preference optimization"}, {"arxiv_id": "2505.22967", "title": "MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming", "year": 2025, "context": "mermaidflow: redefining agentic workflow generation via safety-constrained evolutionary programming"}, {"arxiv_id": null, "title": "MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework", "year": 0, "context": "metagpt: meta programming for a multi-agent collaborative framework"}, {"arxiv_id": null, "title": "AutoGen: Enabling next-Gen LLM Applications via Multi-Agent Conversations", "year": 0, "context": "autogen: enabling next-gen llm applications via multi-agent conversations"}, {"arxiv_id": "2407.12821", "title": "AutoFlow: Automated Workflow Generation for Large Language Model Agents", "year": 2024, "context": "autoflow: automated workflow generation for large language model agents"}], "color": "#FF9800"}, {"id": "Agent Collaboration", "category": "Multi-Agent", "how": "Specialized agents handle subtasks -> Combine outputs", "what": "Division of labor for complex tasks", "frequency": 8, "years": [2024, 2025], "yearRange": "2024-2025", "papers": [{"arxiv_id": null, "title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors", "year": 0, "context": "agentverse: facilitating multi-agent collaboration and exploring emergent behaviors"}, {"arxiv_id": null, "title": "A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration", "year": 0, "context": "a dynamic llm-powered agent network for task-oriented agent collaboration"}, {"arxiv_id": null, "title": "Self-Evolving Multi-Agent Collaboration Networks for Software Development", "year": 0, "context": "self-evolving multi-agent collaboration networks for software development"}, {"arxiv_id": "2404.15155", "title": "MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making", "year": 2024, "context": "mdagents: an adaptive collaboration of llms for medical decision-making"}, {"arxiv_id": null, "title": "Self-Evolving Multi-Agent Collaboration Networks for Software Development", "year": 0, "context": "self-evolving multi-agent collaboration networks for software development"}, {"arxiv_id": "2505.15047", "title": "PiFlow: Principle\u2011aware Scientific Discovery with Multi\u2011Agent Collaboration", "year": 2025, "context": "piflow: principle\u2011aware scientific discovery with multi\u2011agent collaboration"}, {"arxiv_id": null, "title": "Self-Evolving Multi-Agent Collaboration Networks for Software Development", "year": 0, "context": "self-evolving multi-agent collaboration networks for software development"}, {"arxiv_id": "2503.01935", "title": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents", "year": 2025, "context": "multiagentbench: evaluating the collaboration and competition of llm agents"}], "color": "#FF9800"}, {"id": "Tool Learning", "category": "Tool Evolution", "how": "Fine-tune on tool-use demonstrations", "what": "Learns tool selection and invocation", "frequency": 6, "years": [2023, 2024, 2025], "yearRange": "2023-2025", "papers": [{"arxiv_id": "2305.18752", "title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction", "year": 2023, "context": "gpt4tools: teaching large language model to use tools via self-instruction"}, {"arxiv_id": "2307.16789", "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "year": 2023, "context": "toolllm: facilitating large language models to master 16000+ real-world apis"}, {"arxiv_id": "2403.04746", "title": "LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error", "year": 2024, "context": "llms in the imaginarium: tool learning through simulated trial and error"}, {"arxiv_id": "2308.14034", "title": "Confucius: Iterative tool learning from introspection feedback by easy-to-difficult curriculum", "year": 2023, "context": "confucius: iterative tool learning from introspection feedback by easy-to-difficult curriculum"}, {"arxiv_id": "2504.13958", "title": "ToolRL: Reward is All Tool Learning Needs", "year": 2025, "context": "toolrl: reward is all tool learning needs"}, {"arxiv_id": "2307.16789", "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "year": 2023, "context": "toolllm: facilitating large language models to master 16000+ real-world apis"}], "color": "#E91E63"}, {"id": "Self-Play RL", "category": "RL-Based", "how": "Agent plays both sides, learns from game outcomes", "what": "Competitive self-interaction drives improvement", "frequency": 4, "years": [2025], "yearRange": "2025-2025", "papers": [{"arxiv_id": "2505.03335", "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data", "year": 2025, "context": "absolute zero: reinforced self-play reasoning with zero data"}, {"arxiv_id": "2506.24119", "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning", "year": 2025, "context": "spiral: self-play on zero-sum games incentivizes reasoning via multi-agent multi-turn reinforcement learning"}, {"arxiv_id": "2509.25541", "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play", "year": 2025, "context": "vision-zero: scalable vlm self-improvement via strategic gamified self-play"}, {"arxiv_id": "2505.20347", "title": "SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data", "year": 2025, "context": "serl: self-play reinforcement learning for large language models with limited data"}], "color": "#E74C3C"}, {"id": "Process Reward Model", "category": "RL-Based", "how": "Reward each reasoning step, not just final answer", "what": "Dense feedback for better credit assignment", "frequency": 3, "years": [2023, 2024], "yearRange": "2023-2024", "papers": [{"arxiv_id": "2402.00658", "title": "Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing", "year": 2024, "context": "learning planning-based reasoning by trajectories collection and process reward synthesizing"}, {"arxiv_id": "2312.08935", "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations", "year": 2023, "context": "math-shepherd: verify and reinforce llms step-by-step without human annotations"}, {"arxiv_id": "2402.00658", "title": "Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing", "year": 2024, "context": "learning planning-based reasoning by trajectories collection and process reward synthesizing"}], "color": "#E74C3C"}, {"id": "STaR", "category": "Bootstrapping", "how": "Generate rationales -> Filter correct -> Fine-tune on them", "what": "Uses own correct solutions as training data", "frequency": 3, "years": [2022, 2025], "yearRange": "2022-2025", "papers": [{"arxiv_id": "2203.14465", "title": "STaR : Bootstrapping reasoning with reasoning", "year": 2022, "context": "star : bootstrapping reasoning with reasoning"}, {"arxiv_id": "2503.04625", "title": "START: Self\u2011taught Reasoner with Tools", "year": 2025, "context": ""}, {"arxiv_id": "2505.16410", "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "year": 2025, "context": "tool-star: empowering llm-brained multi-tool reasoner via reinforcement learning"}], "color": "#F39C12"}, {"id": "Tool Discovery", "category": "Tool Evolution", "how": "Search available tools -> Plan sequence -> Execute", "what": "Dynamically finds and chains tools", "frequency": 3, "years": [2023, 2024, 2025], "yearRange": "2023-2025", "papers": [{"arxiv_id": "2310.13227", "title": "ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search", "year": 2023, "context": "toolchain*: efficient action space navigation in large language models with a* search"}, {"arxiv_id": "2406.03807", "title": "Tool-Planner: Task Planning with Clusters across Multiple Tools", "year": 2024, "context": "tool-planner: task planning with clusters across multiple tools"}, {"arxiv_id": "2506.01056", "title": "MCP-Zero: Active Tool Discovery for Autonomous LLM Agents", "year": 2025, "context": "mcp-zero: active tool discovery for autonomous llm agents"}], "color": "#E91E63"}, {"id": "Self-Debug", "category": "Feedback-Based", "how": "Execute code -> Read errors -> Fix bugs iteratively", "what": "Uses execution feedback to correct code", "frequency": 2, "years": [2023, 2025], "yearRange": "2023-2025", "papers": [{"arxiv_id": "2304.05128", "title": "Teaching Large Language Models to Self-Debug", "year": 2023, "context": "teaching large language models to self-debug"}, {"arxiv_id": "2502.02928", "title": "Large Language Model Guided Self-Debugging Code Generation", "year": 2025, "context": "large language model guided self-debugging code generation"}], "color": "#3498DB"}, {"id": "Tree-of-Thought", "category": "Search-Based", "how": "Branch into paths -> Evaluate -> Prune bad branches", "what": "Explores reasoning as tree with backtracking", "frequency": 2, "years": [2023, 2025], "yearRange": "2023-2025", "papers": [{"arxiv_id": "2305.10601", "title": "Tree of thoughts: Deliberate problem solving with large language models", "year": 2023, "context": "tree of thoughts: deliberate problem solving with large language models"}, {"arxiv_id": "2507.21836", "title": "AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning", "year": 2025, "context": "autotir: autonomous tools integrated reasoning via reinforcement learning"}], "color": "#9B59B6"}, {"id": "Beam Search", "category": "Search-Based", "how": "Maintain top-k candidates at each step -> Expand best", "what": "Higher quality than greedy decoding", "frequency": 2, "years": [2023, 2024], "yearRange": "2023-2024", "papers": [{"arxiv_id": "2401.17686", "title": "Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning", "year": 2024, "context": "deductive beam search: decoding deducible rationale for chain-of-thought reasoning"}, {"arxiv_id": "2305.03495", "title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search", "year": 2023, "context": "automatic prompt optimization with \"gradient descent\" and beam search"}], "color": "#9B59B6"}, {"id": "Synthetic Data Gen", "category": "Bootstrapping", "how": "LLM generates examples -> Filter -> Train on them", "what": "Creates training corpus without human annotation", "frequency": 2, "years": [2024, 2025], "yearRange": "2024-2025", "papers": [{"arxiv_id": "2405.14333", "title": "DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data", "year": 2024, "context": "deepseek-prover: advancing theorem proving in llms through large-scale synthetic data"}, {"arxiv_id": "2504.04736", "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use", "year": 2025, "context": "synthetic data generation & multi-step rl for reasoning & tool use"}], "color": "#F39C12"}, {"id": "ToolRL", "category": "Tool Evolution", "how": "Reward for successful tool use -> Learn when/how", "what": "RL-based tool selection optimization", "frequency": 2, "years": [2025], "yearRange": "2025-2025", "papers": [{"arxiv_id": "2504.11536", "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs", "year": 2025, "context": "retool: reinforcement learning for strategic tool use in llms"}, {"arxiv_id": "2504.13958", "title": "ToolRL: Reward is All Tool Learning Needs", "year": 2025, "context": "toolrl: reward is all tool learning needs"}], "color": "#E91E63"}, {"id": "Workflow Memory", "category": "Memory Evolution", "how": "Store successful action sequences -> Retrieve for similar tasks", "what": "Remembers proven procedures", "frequency": 2, "years": [2024, 2025], "yearRange": "2024-2025", "papers": [{"arxiv_id": "2409.07429", "title": "Agent Workflow Memory", "year": 2024, "context": ""}, {"arxiv_id": "2502.04306", "title": "ScoreFlow: Mastering LLM Agent Workflows via Score-Based Preference Optimization", "year": 2025, "context": "scoreflow: mastering llm agent workflows via score-based preference optimization"}], "color": "#00BCD4"}, {"id": "Compressive Memory", "category": "Memory Evolution", "how": "Compress long context into retrievable representations", "what": "Enables long-context understanding", "frequency": 2, "years": [2024], "yearRange": "2024-2024", "papers": [{"arxiv_id": "2402.09727", "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts", "year": 2024, "context": "a human-inspired reading agent with gist memory of very long contexts"}, {"arxiv_id": "2402.11975", "title": "Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations", "year": 2024, "context": "compress to impress: unleashing the potential of compressive memory in real-world long-term conversations"}], "color": "#00BCD4"}, {"id": "PPO Fine-Tuning", "category": "RL-Based", "how": "Constrained policy updates based on reward signal", "what": "Stable RL training that prevents catastrophic policy changes", "frequency": 1, "years": [2023], "yearRange": "2023-2023", "papers": [{"arxiv_id": "2308.02151", "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization", "year": 2023, "context": "retroformer: retrospective large language agents with policy gradient optimization"}], "color": "#E74C3C"}, {"id": "RLHF", "category": "RL-Based", "how": "Train reward model on preferences, then optimize policy", "what": "Aligns model with human preferences through feedback", "frequency": 1, "years": [2024], "yearRange": "2024-2024", "papers": [{"arxiv_id": "2405.17346", "title": "Prompt Optimization with Human Feedback", "year": 2024, "context": ""}], "color": "#E74C3C"}, {"id": "Self-Rewarding", "category": "RL-Based", "how": "Model generates its own reward signal for training", "what": "Eliminates need for external reward model", "frequency": 1, "years": [2024], "yearRange": "2024-2024", "papers": [{"arxiv_id": "2401.10020", "title": "Self-Rewarding Language Models", "year": 2024, "context": ""}], "color": "#E74C3C"}, {"id": "Self-Refine", "category": "Feedback-Based", "how": "Generate -> Critique -> Refine loop until quality met", "what": "LLM improves its own output through self-critique", "frequency": 1, "years": [2023], "yearRange": "2023-2023", "papers": [{"arxiv_id": "2303.17651", "title": "Self-Refine: Iterative Refinement with Self-Feedback", "year": 2023, "context": "self-refine: iterative refinement with self-feedback"}], "color": "#3498DB"}, {"id": "Self-Edit", "category": "Feedback-Based", "how": "Identify fault locations -> Apply targeted edits", "what": "Locates errors and makes minimal corrections", "frequency": 1, "years": [2023], "yearRange": "2023-2023", "papers": [{"arxiv_id": "2305.04087", "title": "Self-Edit: Fault-Aware Code Editor for Code Generation", "year": 2023, "context": "self-edit: fault-aware code editor for code generation"}], "color": "#3498DB"}, {"id": "MCTS Reasoning", "category": "Search-Based", "how": "Random rollouts -> Backpropagate values -> Guide search", "what": "Balances exploration and exploitation in reasoning", "frequency": 1, "years": [2025], "yearRange": "2025-2025", "papers": [{"arxiv_id": "2502.12468", "title": "MCTS-Judge: Test-Time Scaling in LLM-as-a-Judge for Code Correctness Evaluation", "year": 2025, "context": "mcts-judge: test-time scaling in llm-as-a-judge for code correctness evaluation"}], "color": "#9B59B6"}, {"id": "Self-Consistency", "category": "Search-Based", "how": "Sample multiple solutions -> Aggregate via voting", "what": "Reduces variance by selecting most consistent answer", "frequency": 1, "years": [2022], "yearRange": "2022-2022", "papers": [{"arxiv_id": "2203.11171", "title": "Self-consistency improves chain of thought reasoning in language models", "year": 2022, "context": "self-consistency improves chain of thought reasoning in language models"}], "color": "#9B59B6"}, {"id": "Graph-of-Thought", "category": "Search-Based", "how": "Reasoning as graph with arbitrary connections", "what": "More flexible than tree structure", "frequency": 1, "years": [2023], "yearRange": "2023-2023", "papers": [{"arxiv_id": "2308.09687", "title": "Graph of thoughts: Solving elaborate problems with large language models", "year": 2023, "context": "graph of thoughts: solving elaborate problems with large language models"}], "color": "#9B59B6"}, {"id": "Promptbreeder", "category": "Evolutionary Prompt", "how": "Self-referential mutation of prompts and mutation operators", "what": "Meta-evolution of both prompts and evolution strategies", "frequency": 1, "years": [2023], "yearRange": "2023-2023", "papers": [{"arxiv_id": "2309.16797", "title": "Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution", "year": 2023, "context": "promptbreeder: self-referential self-improvement via prompt evolution"}], "color": "#2ECC71"}, {"id": "EvoPrompt", "category": "Evolutionary Prompt", "how": "Population of prompts -> Mutation/Crossover -> Selection", "what": "Genetic algorithm operators for prompt optimization", "frequency": 1, "years": [2023], "yearRange": "2023-2023", "papers": [{"arxiv_id": "2309.08532", "title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers", "year": 2023, "context": "evoprompt: connecting llms with evolutionary algorithms yields powerful prompt optimizers"}], "color": "#2ECC71"}, {"id": "GPS Prompt", "category": "Evolutionary Prompt", "how": "Genetic search over discrete prompt space", "what": "Few-shot learning via evolved prompts", "frequency": 1, "years": [2022], "yearRange": "2022-2022", "papers": [{"arxiv_id": "2210.17041", "title": "GPS: Genetic Prompt Search for Efficient Few-shot Learning", "year": 2022, "context": "gps: genetic prompt search for efficient few-shot learning"}], "color": "#2ECC71"}, {"id": "OPRO", "category": "Evolutionary Prompt", "how": "LLM generates and evaluates prompt candidates", "what": "Uses LLM itself to optimize prompts", "frequency": 1, "years": [2023], "yearRange": "2023-2023", "papers": [{"arxiv_id": "2309.08532", "title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers", "year": 2023, "context": "evoprompt: connecting llms with evolutionary algorithms yields powerful prompt optimizers"}], "color": "#2ECC71"}, {"id": "GrIPS", "category": "Evolutionary Prompt", "how": "Iterative edits based on task feedback", "what": "Gradient-free prompt optimization via edits", "frequency": 1, "years": [2022], "yearRange": "2022-2022", "papers": [{"arxiv_id": "2203.07281", "title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models", "year": 2022, "context": "grips: gradient-free, edit-based instruction search for prompting large language models"}], "color": "#2ECC71"}, {"id": "TextGrad", "category": "Text Gradient", "how": "Natural language feedback as gradient signal", "what": "Backpropagates textual critiques to improve prompts", "frequency": 1, "years": [2024], "yearRange": "2024-2024", "papers": [{"arxiv_id": "2406.07496", "title": "TextGrad: Automatic \"Differentiation\" via Text", "year": 2024, "context": "textgrad: automatic \"differentiation\" via text"}], "color": "#1ABC9C"}, {"id": "Semantic Backprop", "category": "Text Gradient", "how": "Propagate semantic feedback through computation graph", "what": "Gradient descent analog for language systems", "frequency": 1, "years": [2024], "yearRange": "2024-2024", "papers": [{"arxiv_id": "2412.03624", "title": "How to Correctly do Semantic Backpropagation on Language-based Agentic Systems", "year": 2024, "context": "how to correctly do semantic backpropagation on language-based agentic systems"}], "color": "#1ABC9C"}, {"id": "GRAD-SUM", "category": "Text Gradient", "how": "Summarize gradients from multiple examples", "what": "Efficient prompt optimization via gradient aggregation", "frequency": 1, "years": [2024], "yearRange": "2024-2024", "papers": [{"arxiv_id": "2407.12865", "title": "GRAD-SUM: Leveraging Gradient Summarization for Optimal Prompt Engineering", "year": 2024, "context": "grad-sum: leveraging gradient summarization for optimal prompt engineering"}], "color": "#1ABC9C"}, {"id": "Self-Instruct", "category": "Bootstrapping", "how": "Bootstrap from seeds -> Generate instructions -> Train", "what": "Creates instruction data from minimal examples", "frequency": 1, "years": [2023], "yearRange": "2023-2023", "papers": [{"arxiv_id": "2305.18752", "title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction", "year": 2023, "context": "gpt4tools: teaching large language model to use tools via self-instruction"}], "color": "#F39C12"}, {"id": "ReST", "category": "Bootstrapping", "how": "Generate -> Filter by reward -> Fine-tune iteratively", "what": "Combines self-training with reward filtering", "frequency": 1, "years": [2024], "yearRange": "2024-2024", "papers": [{"arxiv_id": "2412.09078", "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning", "year": 2024, "context": "forest-of-thought: scaling test-time compute for enhancing llm reasoning"}], "color": "#F39C12"}, {"id": "Tool Creation", "category": "Tool Evolution", "how": "Agent writes code to create new tools", "what": "Expands capability by generating tool functions", "frequency": 1, "years": [2023], "yearRange": "2023-2023", "papers": [{"arxiv_id": "2305.14318", "title": "CREATOR : Tool creation for disentangling abstract and concrete reasoning of large language model", "year": 2023, "context": "creator : tool creation for disentangling abstract and concrete reasoning of large language model"}], "color": "#E91E63"}, {"id": "Episodic Memory", "category": "Memory Evolution", "how": "Store episodes with outcomes -> Query and apply lessons", "what": "Long-term storage of experiences", "frequency": 1, "years": [2023], "yearRange": "2023-2023", "papers": [{"arxiv_id": "2305.10250", "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory", "year": 2023, "context": "memorybank: enhancing large language models with long-term memory"}], "color": "#00BCD4"}, {"id": "Multi-Agent Debate", "category": "Multi-Agent", "how": "Agents argue positions -> Refine -> Converge", "what": "Adversarial discussion improves reasoning", "frequency": 1, "years": [2024], "yearRange": "2024-2024", "papers": [{"arxiv_id": "2405.20267", "title": "Auto-Arena: Automating LLM Evaluations with Agent Peer Debate and Committee Voting", "year": 2024, "context": "auto-arena: automating llm evaluations with agent peer debate and committee voting"}], "color": "#FF9800"}, {"id": "MAS Architecture Search", "category": "Multi-Agent", "how": "Automatically design multi-agent system topology", "what": "Learns optimal agent configurations", "frequency": 1, "years": [], "yearRange": "?", "papers": [{"arxiv_id": null, "title": "Multi-Agent Architecture Search via Agentic Supernet", "year": 0, "context": "multi-agent architecture search via agentic supernet"}], "color": "#FF9800"}, {"id": "Symbolic Learning", "category": "Multi-Agent", "how": "Agents learn symbolic rules from interactions", "what": "Enables evolution beyond training distribution", "frequency": 1, "years": [2024], "yearRange": "2024-2024", "papers": [{"arxiv_id": "2406.18532", "title": "Symbolic Learning Enables Self-Evolving Agents", "year": 2024, "context": "symbolic learning enables self-evolving agents"}], "color": "#FF9800"}],
            links: [{"source": "EvoPrompt", "target": "OPRO", "strength": 1.0}, {"source": "Workflow Optimization", "target": "Workflow Memory", "strength": 0.3333333333333333}, {"source": "Agent Collaboration", "target": "MAS Architecture Search", "strength": 0.25}, {"source": "Tool Learning", "target": "Self-Instruct", "strength": 0.2}, {"source": "Workflow Optimization", "target": "MAS Architecture Search", "strength": 0.16666666666666666}, {"source": "Tool Learning", "target": "ToolRL", "strength": 0.16666666666666666}, {"source": "Workflow Optimization", "target": "Agent Collaboration", "strength": 0.1111111111111111}]
        };
        const years = [2022, 2023, 2024, 2025];
        const categoryColors = {"RL-Based": "#E74C3C", "Feedback-Based": "#3498DB", "Search-Based": "#9B59B6", "Evolutionary Prompt": "#2ECC71", "Text Gradient": "#1ABC9C", "Bootstrapping": "#F39C12", "Tool Evolution": "#E91E63", "Memory Evolution": "#00BCD4", "Multi-Agent": "#FF9800"};
        const categories = Object.keys(categoryColors);
        const categoryNodes = categories.map(cat => ({
            id: cat,
            category: cat,
            isCategory: true,
            frequency: 10,
            years: years,
            color: categoryColors[cat]
        }));
        const categoryLinks = data.nodes.map(n => ({
            source: n.id,
            target: n.category,
            strength: 0.9,
            type: "category"
        }));
        const allNodes = [...categoryNodes, ...data.nodes];
        const allLinks = [...data.links, ...categoryLinks];
        const nodeById = new Map(allNodes.map(n => [n.id, n]));

        let isPlaying = false;
        let playInterval = null;
        let currentYearIdx = -1;
        let activeCategory = null;

        // Build legend
        const legendEl = document.getElementById("legend");
        Object.entries(categoryColors).forEach(([cat, color]) => {
            const count = data.nodes.filter(n => n.category === cat).length;
            if (count > 0) {
                const item = document.createElement("div");
                item.className = "legend-item";
                item.innerHTML = `<div class="legend-color" style="background:${color}"></div>${cat} (${count})`;
                item.onclick = () => filterByCategory(cat, item);
                legendEl.appendChild(item);
            }
        });

        function filterByCategory(cat, el) {
            if (activeCategory === cat) {
                activeCategory = null;
                document.querySelectorAll('.legend-item').forEach(e => e.classList.remove('active'));
                node.classed("dimmed", false);
                link.classed("dimmed", false);
            } else {
                activeCategory = cat;
                document.querySelectorAll('.legend-item').forEach(e => e.classList.remove('active'));
                el.classList.add('active');
                node.classed("dimmed", d => !d.isCategory && d.category !== cat);
                link.classed("dimmed", l => {
                    const s = nodeById.get(l.source.id || l.source);
                    const t = nodeById.get(l.target.id || l.target);
                    if (!s || !t) return true;
                    return s.category !== cat && t.category !== cat;
                });
            }
        }

        const svg = d3.select("#graph");
        const width = svg.node().parentNode.clientWidth;
        const height = svg.node().parentNode.clientHeight;
        svg.attr("viewBox", [0, 0, width, height]);

        const g = svg.append("g");
        const zoom = d3.zoom().scaleExtent([0.3, 4]).on("zoom", e => g.attr("transform", e.transform));
        svg.call(zoom);

        // Category centers to keep related methods clustered (reduce scattering)
        const centerRadius = Math.min(width, height) * 0.28;
        const categoryCenters = {};
        categories.forEach((cat, i) => {
            const angle = (i / categories.length) * Math.PI * 2;
            categoryCenters[cat] = {
                x: width / 2 + Math.cos(angle) * centerRadius,
                y: height / 2 + Math.sin(angle) * centerRadius
            };
        });

        // V3-style force simulation: coherent clusters with stronger local ties
        const simulation = d3.forceSimulation(allNodes)
            .force("link", d3.forceLink(allLinks).id(d => d.id)
                .distance(d => 120 - Math.min(1, d.strength ?? 0.5) * 70)
                .strength(d => 0.3 + (d.strength ?? 0.5) * 0.7))
            .force("charge", d3.forceManyBody().strength(-160))
            .force("center", d3.forceCenter(width/2, height/2))
            .force("collision", d3.forceCollide().radius(d => Math.sqrt(d.frequency)*4 + 14))
            .force("categoryX", d3.forceX(d => (categoryCenters[d.category]?.x ?? width/2)).strength(0.18))
            .force("categoryY", d3.forceY(d => (categoryCenters[d.category]?.y ?? height/2)).strength(0.18));

        const link = g.append("g").selectAll("line").data(allLinks).join("line")
            .attr("class", d => d.type === "category" ? "link category-link" : "link")
            .attr("stroke-opacity", d => 0.12 + (d.strength ?? 0.4)*0.3);

        const node = g.append("g").selectAll("g").data(allNodes).join("g")
            .attr("class", d => d.isCategory ? "node category-node" : "node")
            .call(d3.drag()
                .on("start", (e,d) => { if(!e.active) simulation.alphaTarget(0.3).restart(); d.fx=d.x; d.fy=d.y; })
                .on("drag", (e,d) => { d.fx=e.x; d.fy=e.y; })
                .on("end", (e,d) => { if(!e.active) simulation.alphaTarget(0); d.fx=null; d.fy=null; }));

        // Sized nodes based on frequency
        node.append("circle")
            .attr("r", d => d.isCategory ? 22 : Math.sqrt(d.frequency)*4 + 5)
            .attr("fill", d => d.color)
            .attr("stroke", d => d.color);

        node.append("text")
            .text(d => d.id)
            .attr("x", d => d.isCategory ? 0 : Math.sqrt(d.frequency)*4 + 8)
            .attr("y", d => d.isCategory ? 4 : 3)
            .attr("text-anchor", d => d.isCategory ? "middle" : "start");

        const tooltip = d3.select("#tooltip");

        node.on("mouseover", (e, d) => {
            if (d.isCategory) return;
            tooltip.classed("visible", true)
                .html(`<h5>${d.id}</h5><div class="cat">${d.category}</div><div class="how">${d.how}</div><div class="freq">${d.frequency} papers | ${d.yearRange}</div>`)
                .style("left", (e.pageX+12)+"px").style("top", (e.pageY-8)+"px");
            link.classed("highlighted", l => l.source.id===d.id || l.target.id===d.id);
        }).on("mouseout", () => {
            tooltip.classed("visible", false);
            link.classed("highlighted", false);
        }).on("click", (e, d) => {
            if (d.isCategory) return;
            node.classed("selected", false);
            d3.select(e.currentTarget).classed("selected", true);
            showDetail(d);
        });

        function showDetail(d) {
            const panel = document.getElementById("detailPanel");
            const papers = d.papers.map(p => `
                <div class="paper-item">
                    <div class="paper-title">${p.title || 'Untitled'}</div>
                    <div class="paper-year">${p.year||'?'} - arXiv:${p.arxiv_id}</div>
                </div>`).join('');

            panel.innerHTML = `
                <div class="method-card">
                    <div class="method-name">${d.id}</div>
                    <div class="method-category" style="background:${d.color}">${d.category}</div>
                    <div class="info-box how">
                        <h4>How It Works</h4>
                        <p>${d.how}</p>
                    </div>
                    <div class="info-box what">
                        <h4>What It Does</h4>
                        <p>${d.what}</p>
                    </div>
                    <div class="stats-row">
                        <div class="stat-box"><div class="num">${d.frequency}</div><div class="lbl">Papers</div></div>
                        <div class="stat-box"><div class="num">${d.yearRange}</div><div class="lbl">Years</div></div>
                    </div>
                    <div class="papers-title">Papers Using This Method</div>
                    ${papers}
                </div>`;
        }

        simulation.on("tick", () => {
            link.attr("x1",d=>d.source.x).attr("y1",d=>d.source.y).attr("x2",d=>d.target.x).attr("y2",d=>d.target.y);
            node.attr("transform", d=>`translate(${d.x},${d.y})`);
        });

        function resetZoom() { svg.transition().duration(600).call(zoom.transform, d3.zoomIdentity); }

        // Year animation
        const playBtn = document.getElementById("playBtn");
        const yearDisplay = document.getElementById("yearDisplay");

        function highlightYear(year) {
            if (year === null) {
                yearDisplay.textContent = "ALL";
                node.classed("dimmed", false).classed("highlighted", false);
                link.classed("dimmed", false);
            } else {
                yearDisplay.textContent = year;
                node.classed("dimmed", d => !d.isCategory && !d.years.includes(year));
                node.classed("highlighted", d => !d.isCategory && d.years.includes(year));
                link.classed("dimmed", l => {
                    const s = nodeById.get(l.source.id || l.source);
                    const t = nodeById.get(l.target.id || l.target);
                    if (!s || !t) return true;
                    if (s.isCategory || t.isCategory) return false;
                    return !s.years.includes(year) || !t.years.includes(year);
                });
            }
        }

        playBtn.addEventListener("click", () => {
            if (isPlaying) {
                clearInterval(playInterval);
                isPlaying = false;
                playBtn.innerHTML = '<svg viewBox="0 0 24 24" fill="currentColor"><path d="M8 5v14l11-7z"/></svg>';
                highlightYear(null);
                currentYearIdx = -1;
            } else {
                isPlaying = true;
                playBtn.innerHTML = '<svg viewBox="0 0 24 24" fill="currentColor"><rect x="6" y="5" width="4" height="14"/><rect x="14" y="5" width="4" height="14"/></svg>';
                currentYearIdx = 0;
                highlightYear(years[currentYearIdx]);
                playInterval = setInterval(() => {
                    currentYearIdx++;
                    if (currentYearIdx >= years.length) {
                        clearInterval(playInterval);
                        isPlaying = false;
                        playBtn.innerHTML = '<svg viewBox="0 0 24 24" fill="currentColor"><path d="M8 5v14l11-7z"/></svg>';
                        setTimeout(() => highlightYear(null), 1200);
                        currentYearIdx = -1;
                    } else {
                        highlightYear(years[currentYearIdx]);
                    }
                }, 1800);
            }
        });
    </script>
</body>
</html>